[{"path":"/articles/daiquiri.html","id":"data-format","dir":"Articles","previous_headings":"","what":"Data format","title":"Walkthrough for the daiquiri package","text":"Data must tabular format, data frame. row represent single “event” visit hospital medical test result drug prescription. One column must contain “event date” (aka “timepoint”) row, columns containing associated values event, type visit, name medical test, quantity drug prescribed. order package detect non-conformant values numeric datetime fields, present data frame raw character format. function called read_data() supplied assist , read rectangular data text file automatically character type. Dates expected format YYYY-MM-DD YYYY-MM-DD HH:MM:SS, though range date formats can also accepted (see later). User-specified locales currently supported. Example: example_prescriptions dataset provided package csv file, contains (synthetic) examples antibiotic prescriptions given hospital period year, one row per prescription. contains 8 columns: PrescriptionID - uniqueidentifier row PrescriptionDate - date prescription given AdmissionDate - date patient admitted hospital Drug - name antibiotic prescribed Dose - size dose prescribed DoseUnit - units dose prescribed PatientID - unique identifier patient Location - location patient missing values represented string ‘NULL’. String values treated missing values can specified data processed, using na parameter.","code":"# first, attach the package if you haven't already library(daiquiri)  # this is where the example file is located path <- system.file(\"extdata\", \"example_prescriptions.csv\", package = \"daiquiri\")  # load the data into a data.frame without doing any datatype conversion example_prescriptions <- read_data(   path,   delim = \",\",   col_names = TRUE,   show_progress = FALSE )  head(example_prescriptions) #> # A tibble: 6 × 8 #>   PrescriptionID PrescriptionDate   AdmissionDate Drug  Dose  DoseUnit PatientID #>   <chr>          <chr>              <chr>         <chr> <chr> <chr>    <chr>     #> 1 6000           2021-01-01 00:00:… 2020-12-31    Ceft… 500   mg       4993679   #> 2 6001           NULL               2020-12-31    Fluc… 1000  mg       819452    #> 3 6002           NULL               2020-12-30    Teic… 400   mg       275597    #> 4 6003           2021-01-01 01:00:… 2020-12-31    Fluc… 1000  NULL     819452    #> 5 6004           2021-01-01 02:00:… 2020-12-20    Fluc… 1000  NULL     528071    #> 6 6005           2021-01-01 03:00:… 2020-12-30    Co-a… 1.2   g        1001434   #> # … with 1 more variable: Location <chr>"},{"path":"/articles/daiquiri.html","id":"specification-of-data-field-types","dir":"Articles","previous_headings":"","what":"Specification of data field types","title":"Walkthrough for the daiquiri package","text":"must specify type data expected column, e.g. date, number, nominal category. package use information calculate number values expected type, decide summary functions use time series’. One one column must chosen “timepoint” field (though run package using different column timepoint field choose). column used independent time variable x-axis time series plot. list possible field types columns. Different time series generated depending type field. types time series generated number values present, well number percentage missing values. ft_timepoint() - identifies data field used independent time variable (form x-axis time series plots). one one specified. default, time series also created number percentage values contain time portion, though can switched includes_time parameter set FALSE (e.g. already know advance time portions values). NOTE: Time series’ created missing values assigned date. Instead, total number missing values included summary table. ft_uniqueidentifier() - identifies data fields contain (usually computer-generated) identifier entity, e.g. patient. need unique within dataset. Values treated strings, additional time series created min_length, max_length, mean_length string. ft_categorical() - identifies data fields treated categorical. Values treated strings. Additional time series created number “distinct” values, aggregate_by_each_category parameter set TRUE, time series created number percentage values within distinct subcategory value. ft_numeric() - identifies data fields contain numeric values treated continuous. Additional time series created min, max, mean, median value, number percentage non-conformant values. ft_datetime() - identifies data fields contain date (optionally time) values treated continuous. Additional time series created min, max, mean value, number percentage non-conformant values. default, time series also created number percentage values contain time portion, though can switched includes_time parameter set FALSE (e.g. already know advance time portions values). ft_freetext() - identifies data fields contain free text values. presence/missingness evaluated. ft_simple() - identifies data fields want presence/missingness evaluated (necessarily free text). ft_ignore() - identifies data fields ignored. loaded. Lastly, number time series generated dataset whole, namely: number percentage presence duplicate records (.e. entire row another row dataset) number records (duplicates removed) total number percentage missing non-conformant values (relevant) across data fields NOTES: ft_timepoint() ft_datetime() field types accept format parameter values column YYYY-MM-DD YYYY-MM-DD HH:MM:SS format. must follow col_datetime format specifications readr package, e.g. “%d/%m/%Y”. format supplied, must match complete string. User-specified locales currently supported. data already aggregated (e.g. one column contains date columns contain number inpatient admissions, outpatient appointments, emergency department attendances date), can still use package restrict ft_timepoint(), ft_numeric(), ft_simple() field types. Example: example_prescriptions dataset , use PrescriptionDate timepoint field, specify columns follows: just one many different possible choices, example another option use AdmissionDate timepoint field instead, set Drug column categorical. decide set time series want created column. TIP: lot columns dataset can use print_field_types_template() function print template specification console can copy edit.","code":"# set up a field_types specification for use later fts <- field_types(   PrescriptionID = ft_uniqueidentifier(),   PrescriptionDate = ft_timepoint(),   AdmissionDate = ft_datetime(includes_time = FALSE),   Drug = ft_freetext(),   Dose = ft_numeric(),   DoseUnit = ft_categorical(),   PatientID = ft_ignore(),   Location = ft_categorical(aggregate_by_each_category = TRUE) )"},{"path":"/articles/daiquiri.html","id":"generating-a-data-quality-report","dir":"Articles","previous_headings":"","what":"Generating a data quality report","title":"Walkthrough for the daiquiri package","text":"simplest way create data quality report use create_report() function. successful, function return list containing information relating supplied parameters well resulting source_data aggregated_data objects, can reused create reports without needing run everything . point need decide level aggregation granularity want use. Options daily/weekly/monthly/quarterly/yearly. Smaller aggregation granularities (e.g. day week) provide detail, data sparser might want use larger granularity. Time series created according specified field types, aggregating values records whose timepoint value lies within relevant day/week/month etc. also need decide save report, optionally specify filename (excluding file extension). filename can contain alphanumeric, - _ characters. filename supplied, one automatically generated. can also optionally specify short description dataset, appear report. specify log directory, details processing steps saved text file. can choose override existing column names dataframe field_types() specification setting override_column_names parameter TRUE, case must specified correct order. override_column_names = FALSE (default) names field_types() specification must match exactly dataframe. Example: example_prescriptions dataset , set aggregation granularity day, save report current directory.","code":"daiq_obj <- create_report(   df = example_prescriptions,   field_types = fts,   override_column_names = FALSE,   na = c(\"\", \"NULL\"),   dataset_description = \"Example prescription data\",   aggregation_timeunit = \"day\",   save_directory = \".\",   save_filename = \"example_prescriptions_report\",   show_progress = TRUE,   log_directory = NULL )"},{"path":"/articles/daiquiri.html","id":"contents-of-the-report","dir":"Articles","previous_headings":"","what":"Contents of the report","title":"Walkthrough for the daiquiri package","text":"html reports created package can opened browser /attached email, contain three main tabs:","code":""},{"path":"/articles/daiquiri.html","id":"source-data","dir":"Articles","previous_headings":"Contents of the report","what":"Source data","title":"Walkthrough for the daiquiri package","text":"contains child tabs provide overall summary data imported, plus complete list validation warnings (values identified non-conformant)","code":""},{"path":"/articles/daiquiri.html","id":"aggregated-data","dir":"Articles","previous_headings":"Contents of the report","what":"Aggregated data","title":"Walkthrough for the daiquiri package","text":"contains child tabs display plots showing overall numbers records, missing values, non-conformant values, duplicate records per timepoint, across dataset whole. Fields timepoints values applicable coloured grey.","code":""},{"path":"/articles/daiquiri.html","id":"individual-data-fields","dir":"Articles","previous_headings":"Contents of the report","what":"Individual data fields","title":"Walkthrough for the daiquiri package","text":"contains two levels child tabs. first level consists one tab per data field dataset. Within data field tab, set child tabs displaying plot time series created","code":""},{"path":"/articles/daiquiri.html","id":"advanced-usage-details","dir":"Articles","previous_headings":"","what":"Advanced usage details","title":"Walkthrough for the daiquiri package","text":"simplest way use package use create_report() function described , loads data, aggregates , generates report one go. However, large dataset want create reports multiple aggregation granularities without re-loading data time, may prefer stages. example use intermediate functions objects achieve . NOTE: want change field_types() specification data need re-loaded.","code":"# load your dataset into a source_data object prescriptions_source_data <- prepare_data(   example_prescriptions,   fieldtypes = fts,   na = c(\"\", \"NULL\") )  # aggregate the source_data object by desired granularity prescriptions_byday <- aggregate_data(   prescriptions_source_data,   aggregation_timeunit = \"day\" )  # aggregate the same source_data object by a different granularity prescriptions_byweek <- aggregate_data(   prescriptions_source_data,   aggregation_timeunit = \"week\" )  # generate and save the reports report_data(   source_data = prescriptions_source_data,   aggregated_data = prescriptions_byday,   save_directory = \".\",   save_filename = \"example_prescriptions_byday\" )  report_data(   source_data = prescriptions_source_data,   aggregated_data = prescriptions_byweek,   save_directory = \".\",   save_filename = \"example_prescriptions_byweek\" )"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"T. Phuong Quan. Author, maintainer. Jack Cregan. Contributor. University Oxford. Copyright holder. National Institute Health Research (NIHR). Funder.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Quan TP (2022). daiquiri: Data quality reporting temporal datasets. doi:10.5281/zenodo.6334341, R package version 0.7.3.9000, https://github.com/phuongquan/daiquiri.","code":"@Manual{,   title = {daiquiri: Data quality reporting for temporal datasets},   author = {T. Phuong Quan},   doi = {10.5281/zenodo.6334341},   year = {2022},   note = {R package version 0.7.3.9000},   url = {https://github.com/phuongquan/daiquiri}, }"},{"path":[]},{"path":"/CONTRIBUTING.html","id":"reporting-issues","dir":"","previous_headings":"","what":"Reporting issues","title":"Contributing to daiquiri","text":"Please report bugs suggestions opening github issue.","code":""},{"path":"/CONTRIBUTING.html","id":"development-guidelines","dir":"","previous_headings":"","what":"Development guidelines","title":"Contributing to daiquiri","text":"far, daiquiri developed single person yet system place collaboration. ’d like contribute changes please raise issue first can probably use GitHub flow.","code":""},{"path":"/CONTRIBUTING.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"Contributing to daiquiri","text":"Please keep comments respectful relevant.","code":""},{"path":"/index.html","id":"daiquiri","dir":"","previous_headings":"","what":"Data Quality Reporting for Temporal Datasets","title":"Data Quality Reporting for Temporal Datasets","text":"daiquiri package generates data quality reports enable quick visual review temporal shifts record-level data. Time series plots showing aggregated values automatically created data field (column) depending contents (e.g. min/max/mean values numeric data, . distinct values categorical data), well overviews missing values, non-conformant values, duplicated rows. Essentially, takes input :  outputs :  resulting html reports shareable can contribute forming transparent record entire analysis process. designed electronic health records mind, can used type record-level temporal data.","code":""},{"path":"/index.html","id":"why-should-i-use-it","dir":"","previous_headings":"","what":"Why should I use it?","title":"Data Quality Reporting for Temporal Datasets","text":"Large routinely-collected datasets increasingly used research. However, given data collected operational rather research purposes, greater--usual need checked data quality issues analyses conducted. Events occurring institutional level software updates, new machinery processes can cause temporal artefacts , identified taken account, can lead biased results incorrect conclusions. example, figures show real data large hospital UK, changed time.  first figure shows percentage missing values ‘Duration’ field dataset containing antibiotic prescriptions, second figure shows mean value laboratory tests checking levels ‘creatinine’ blood. can see, points time values shift suddenly unnaturally, indicating something changed way data collected processed. careful researcher needs take sudden changes account, particularly comparing combining data ‘change points’. checks theoretically conducted researcher initial data analysis stage, practice unclear extent actually done, since rarely, ever, reported published papers. increasing drive towards greater transparency reproducibility within scientific community, essential yet often-overlooked part analysis process inevitably begin come greater scrutiny. daiquiri package helps researchers conduct part process thoroughly, consistently transparently, hence increasing quality studies well trust scientific process.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Data Quality Reporting for Temporal Datasets","text":"intention make daiquiri available CRAN , can install current development version GitHub:","code":"# install.packages(\"devtools\") devtools::install_github(\"phuongquan/daiquiri\", build_vignettes = TRUE)"},{"path":"/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"Data Quality Reporting for Temporal Datasets","text":"example report available package website. detailed guidance can found walkthrough vignette:","code":"library(daiquiri)  # load delimited file into a data.frame without doing any datatype conversion path <- system.file(\"extdata\", \"example_prescriptions.csv\", package = \"daiquiri\") raw_data <- read_data(path, show_progress = FALSE)  head(raw_data) ## # A tibble: 6 × 8 ##   PrescriptionID PrescriptionDate   AdmissionDate Drug  Dose  DoseUnit PatientID ##   <chr>          <chr>              <chr>         <chr> <chr> <chr>    <chr>     ## 1 6000           2021-01-01 00:00:… 2020-12-31    Ceft… 500   mg       4993679   ## 2 6001           NULL               2020-12-31    Fluc… 1000  mg       819452    ## 3 6002           NULL               2020-12-30    Teic… 400   mg       275597    ## 4 6003           2021-01-01 01:00:… 2020-12-31    Fluc… 1000  NULL     819452    ## 5 6004           2021-01-01 02:00:… 2020-12-20    Fluc… 1000  NULL     528071    ## 6 6005           2021-01-01 03:00:… 2020-12-30    Co-a… 1.2   g        1001434   ## # … with 1 more variable: Location <chr> # specify the type of data expected in each column of the data.frame fts <- field_types(     PrescriptionID = ft_uniqueidentifier(),     PrescriptionDate = ft_timepoint(),     AdmissionDate = ft_datetime(includes_time = FALSE),     Drug = ft_freetext(),     Dose = ft_numeric(),     DoseUnit = ft_categorical(),     PatientID = ft_ignore(),     Location = ft_categorical(aggregate_by_each_category=TRUE) ) # create a report in the current directory daiq_obj <- create_report(   raw_data,   field_types = fts ) vignette(\"daiquiri\", package = \"daiquiri\")"},{"path":"/index.html","id":"how-to-cite-this-package","dir":"","previous_headings":"","what":"How to cite this package","title":"Data Quality Reporting for Temporal Datasets","text":"Please remember update version number match version used. Quan TP (2022). daiquiri: Data quality reporting temporal datasets. R package version v0.7.0. Zenodo. https://doi.org/10.5281/zenodo.6334341. URL: https://github.com/phuongquan/daiquiri","code":""},{"path":"/index.html","id":"acknowledgements","dir":"","previous_headings":"","what":"Acknowledgements","title":"Data Quality Reporting for Temporal Datasets","text":"work supported National Institute Health Research Health Protection Research Unit (NIHR HPRU) Healthcare Associated Infections Antimicrobial Resistance University Oxford partnership Public Health England (PHE) (NIHR200915), NIHR Oxford Biomedical Research Centre.","code":""},{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Version 3, 29 June 2007Copyright © 2007 Free Software Foundation, Inc. <http://fsf.org/> Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU General Public License","text":"GNU General Public License free, copyleft license software kinds works. licenses software practical works designed take away freedom share change works. contrast, GNU General Public License intended guarantee freedom share change versions program–make sure remains free software users. , Free Software Foundation, use GNU General Public License software; applies also work released way authors. can apply programs, . speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge wish), receive source code can get want , can change software use pieces new free programs, know can things. protect rights, need prevent others denying rights asking surrender rights. Therefore, certain responsibilities distribute copies software, modify : responsibilities respect freedom others. example, distribute copies program, whether gratis fee, must pass recipients freedoms received. must make sure , , receive can get source code. must show terms know rights. Developers use GNU GPL protect rights two steps: (1) assert copyright software, (2) offer License giving legal permission copy, distribute /modify . developers’ authors’ protection, GPL clearly explains warranty free software. users’ authors’ sake, GPL requires modified versions marked changed, problems attributed erroneously authors previous versions. devices designed deny users access install run modified versions software inside , although manufacturer can . fundamentally incompatible aim protecting users’ freedom change software. systematic pattern abuse occurs area products individuals use, precisely unacceptable. Therefore, designed version GPL prohibit practice products. problems arise substantially domains, stand ready extend provision domains future versions GPL, needed protect freedom users. Finally, every program threatened constantly software patents. States allow patents restrict development use software general-purpose computers, , wish avoid special danger patents applied free program make effectively proprietary. prevent , GPL assures patents used render program non-free. precise terms conditions copying, distribution modification follow.","code":""},{"path":[]},{"path":"/LICENSE.html","id":"0-definitions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"0. Definitions","title":"GNU General Public License","text":"“License” refers version 3 GNU General Public License. “Copyright” also means copyright-like laws apply kinds works, semiconductor masks. “Program” refers copyrightable work licensed License. licensee addressed “”. “Licensees” “recipients” may individuals organizations. “modify” work means copy adapt part work fashion requiring copyright permission, making exact copy. resulting work called “modified version” earlier work work “based ” earlier work. “covered work” means either unmodified Program work based Program. “propagate” work means anything , without permission, make directly secondarily liable infringement applicable copyright law, except executing computer modifying private copy. Propagation includes copying, distribution (without modification), making available public, countries activities well. “convey” work means kind propagation enables parties make receive copies. Mere interaction user computer network, transfer copy, conveying. interactive user interface displays “Appropriate Legal Notices” extent includes convenient prominently visible feature (1) displays appropriate copyright notice, (2) tells user warranty work (except extent warranties provided), licensees may convey work License, view copy License. interface presents list user commands options, menu, prominent item list meets criterion.","code":""},{"path":"/LICENSE.html","id":"1-source-code","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"1. Source Code","title":"GNU General Public License","text":"“source code” work means preferred form work making modifications . “Object code” means non-source form work. “Standard Interface” means interface either official standard defined recognized standards body, , case interfaces specified particular programming language, one widely used among developers working language. “System Libraries” executable work include anything, work whole, () included normal form packaging Major Component, part Major Component, (b) serves enable use work Major Component, implement Standard Interface implementation available public source code form. “Major Component”, context, means major essential component (kernel, window system, ) specific operating system () executable work runs, compiler used produce work, object code interpreter used run . “Corresponding Source” work object code form means source code needed generate, install, (executable work) run object code modify work, including scripts control activities. However, include work’s System Libraries, general-purpose tools generally available free programs used unmodified performing activities part work. example, Corresponding Source includes interface definition files associated source files work, source code shared libraries dynamically linked subprograms work specifically designed require, intimate data communication control flow subprograms parts work. Corresponding Source need include anything users can regenerate automatically parts Corresponding Source. Corresponding Source work source code form work.","code":""},{"path":"/LICENSE.html","id":"2-basic-permissions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"2. Basic Permissions","title":"GNU General Public License","text":"rights granted License granted term copyright Program, irrevocable provided stated conditions met. License explicitly affirms unlimited permission run unmodified Program. output running covered work covered License output, given content, constitutes covered work. License acknowledges rights fair use equivalent, provided copyright law. may make, run propagate covered works convey, without conditions long license otherwise remains force. may convey covered works others sole purpose make modifications exclusively , provide facilities running works, provided comply terms License conveying material control copyright. thus making running covered works must exclusively behalf, direction control, terms prohibit making copies copyrighted material outside relationship . Conveying circumstances permitted solely conditions stated . Sublicensing allowed; section 10 makes unnecessary.","code":""},{"path":"/LICENSE.html","id":"3-protecting-users-legal-rights-from-anti-circumvention-law","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"3. Protecting Users’ Legal Rights From Anti-Circumvention Law","title":"GNU General Public License","text":"covered work shall deemed part effective technological measure applicable law fulfilling obligations article 11 WIPO copyright treaty adopted 20 December 1996, similar laws prohibiting restricting circumvention measures. convey covered work, waive legal power forbid circumvention technological measures extent circumvention effected exercising rights License respect covered work, disclaim intention limit operation modification work means enforcing, work’s users, third parties’ legal rights forbid circumvention technological measures.","code":""},{"path":"/LICENSE.html","id":"4-conveying-verbatim-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"4. Conveying Verbatim Copies","title":"GNU General Public License","text":"may convey verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice; keep intact notices stating License non-permissive terms added accord section 7 apply code; keep intact notices absence warranty; give recipients copy License along Program. may charge price price copy convey, may offer support warranty protection fee.","code":""},{"path":"/LICENSE.html","id":"5-conveying-modified-source-versions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"5. Conveying Modified Source Versions","title":"GNU General Public License","text":"may convey work based Program, modifications produce Program, form source code terms section 4, provided also meet conditions: ) work must carry prominent notices stating modified , giving relevant date. b) work must carry prominent notices stating released License conditions added section 7. requirement modifies requirement section 4 “keep intact notices”. c) must license entire work, whole, License anyone comes possession copy. License therefore apply, along applicable section 7 additional terms, whole work, parts, regardless packaged. License gives permission license work way, invalidate permission separately received . d) work interactive user interfaces, must display Appropriate Legal Notices; however, Program interactive interfaces display Appropriate Legal Notices, work need make . compilation covered work separate independent works, nature extensions covered work, combined form larger program, volume storage distribution medium, called “aggregate” compilation resulting copyright used limit access legal rights compilation’s users beyond individual works permit. Inclusion covered work aggregate cause License apply parts aggregate.","code":""},{"path":"/LICENSE.html","id":"6-conveying-non-source-forms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"6. Conveying Non-Source Forms","title":"GNU General Public License","text":"may convey covered work object code form terms sections 4 5, provided also convey machine-readable Corresponding Source terms License, one ways: ) Convey object code , embodied , physical product (including physical distribution medium), accompanied Corresponding Source fixed durable physical medium customarily used software interchange. b) Convey object code , embodied , physical product (including physical distribution medium), accompanied written offer, valid least three years valid long offer spare parts customer support product model, give anyone possesses object code either (1) copy Corresponding Source software product covered License, durable physical medium customarily used software interchange, price reasonable cost physically performing conveying source, (2) access copy Corresponding Source network server charge. c) Convey individual copies object code copy written offer provide Corresponding Source. alternative allowed occasionally noncommercially, received object code offer, accord subsection 6b. d) Convey object code offering access designated place (gratis charge), offer equivalent access Corresponding Source way place charge. need require recipients copy Corresponding Source along object code. place copy object code network server, Corresponding Source may different server (operated third party) supports equivalent copying facilities, provided maintain clear directions next object code saying find Corresponding Source. Regardless server hosts Corresponding Source, remain obligated ensure available long needed satisfy requirements. e) Convey object code using peer--peer transmission, provided inform peers object code Corresponding Source work offered general public charge subsection 6d. separable portion object code, whose source code excluded Corresponding Source System Library, need included conveying object code work. “User Product” either (1) “consumer product”, means tangible personal property normally used personal, family, household purposes, (2) anything designed sold incorporation dwelling. determining whether product consumer product, doubtful cases shall resolved favor coverage. particular product received particular user, “normally used” refers typical common use class product, regardless status particular user way particular user actually uses, expects expected use, product. product consumer product regardless whether product substantial commercial, industrial non-consumer uses, unless uses represent significant mode use product. “Installation Information” User Product means methods, procedures, authorization keys, information required install execute modified versions covered work User Product modified version Corresponding Source. information must suffice ensure continued functioning modified object code case prevented interfered solely modification made. convey object code work section , , specifically use , User Product, conveying occurs part transaction right possession use User Product transferred recipient perpetuity fixed term (regardless transaction characterized), Corresponding Source conveyed section must accompanied Installation Information. requirement apply neither third party retains ability install modified object code User Product (example, work installed ROM). requirement provide Installation Information include requirement continue provide support service, warranty, updates work modified installed recipient, User Product modified installed. Access network may denied modification materially adversely affects operation network violates rules protocols communication across network. Corresponding Source conveyed, Installation Information provided, accord section must format publicly documented (implementation available public source code form), must require special password key unpacking, reading copying.","code":""},{"path":"/LICENSE.html","id":"7-additional-terms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"7. Additional Terms","title":"GNU General Public License","text":"“Additional permissions” terms supplement terms License making exceptions one conditions. Additional permissions applicable entire Program shall treated though included License, extent valid applicable law. additional permissions apply part Program, part may used separately permissions, entire Program remains governed License without regard additional permissions. convey copy covered work, may option remove additional permissions copy, part . (Additional permissions may written require removal certain cases modify work.) may place additional permissions material, added covered work, can give appropriate copyright permission. Notwithstanding provision License, material add covered work, may (authorized copyright holders material) supplement terms License terms: ) Disclaiming warranty limiting liability differently terms sections 15 16 License; b) Requiring preservation specified reasonable legal notices author attributions material Appropriate Legal Notices displayed works containing ; c) Prohibiting misrepresentation origin material, requiring modified versions material marked reasonable ways different original version; d) Limiting use publicity purposes names licensors authors material; e) Declining grant rights trademark law use trade names, trademarks, service marks; f) Requiring indemnification licensors authors material anyone conveys material (modified versions ) contractual assumptions liability recipient, liability contractual assumptions directly impose licensors authors. non-permissive additional terms considered “restrictions” within meaning section 10. Program received , part , contains notice stating governed License along term restriction, may remove term. license document contains restriction permits relicensing conveying License, may add covered work material governed terms license document, provided restriction survive relicensing conveying. add terms covered work accord section, must place, relevant source files, statement additional terms apply files, notice indicating find applicable terms. Additional terms, permissive non-permissive, may stated form separately written license, stated exceptions; requirements apply either way.","code":""},{"path":"/LICENSE.html","id":"8-termination","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"8. Termination","title":"GNU General Public License","text":"may propagate modify covered work except expressly provided License. attempt otherwise propagate modify void, automatically terminate rights License (including patent licenses granted third paragraph section 11). However, cease violation License, license particular copyright holder reinstated () provisionally, unless copyright holder explicitly finally terminates license, (b) permanently, copyright holder fails notify violation reasonable means prior 60 days cessation. Moreover, license particular copyright holder reinstated permanently copyright holder notifies violation reasonable means, first time received notice violation License (work) copyright holder, cure violation prior 30 days receipt notice. Termination rights section terminate licenses parties received copies rights License. rights terminated permanently reinstated, qualify receive new licenses material section 10.","code":""},{"path":"/LICENSE.html","id":"9-acceptance-not-required-for-having-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"9. Acceptance Not Required for Having Copies","title":"GNU General Public License","text":"required accept License order receive run copy Program. Ancillary propagation covered work occurring solely consequence using peer--peer transmission receive copy likewise require acceptance. However, nothing License grants permission propagate modify covered work. actions infringe copyright accept License. Therefore, modifying propagating covered work, indicate acceptance License .","code":""},{"path":"/LICENSE.html","id":"10-automatic-licensing-of-downstream-recipients","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"10. Automatic Licensing of Downstream Recipients","title":"GNU General Public License","text":"time convey covered work, recipient automatically receives license original licensors, run, modify propagate work, subject License. responsible enforcing compliance third parties License. “entity transaction” transaction transferring control organization, substantially assets one, subdividing organization, merging organizations. propagation covered work results entity transaction, party transaction receives copy work also receives whatever licenses work party’s predecessor interest give previous paragraph, plus right possession Corresponding Source work predecessor interest, predecessor can get reasonable efforts. may impose restrictions exercise rights granted affirmed License. example, may impose license fee, royalty, charge exercise rights granted License, may initiate litigation (including cross-claim counterclaim lawsuit) alleging patent claim infringed making, using, selling, offering sale, importing Program portion .","code":""},{"path":"/LICENSE.html","id":"11-patents","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"11. Patents","title":"GNU General Public License","text":"“contributor” copyright holder authorizes use License Program work Program based. work thus licensed called contributor’s “contributor version”. contributor’s “essential patent claims” patent claims owned controlled contributor, whether already acquired hereafter acquired, infringed manner, permitted License, making, using, selling contributor version, include claims infringed consequence modification contributor version. purposes definition, “control” includes right grant patent sublicenses manner consistent requirements License. contributor grants non-exclusive, worldwide, royalty-free patent license contributor’s essential patent claims, make, use, sell, offer sale, import otherwise run, modify propagate contents contributor version. following three paragraphs, “patent license” express agreement commitment, however denominated, enforce patent (express permission practice patent covenant sue patent infringement). “grant” patent license party means make agreement commitment enforce patent party. convey covered work, knowingly relying patent license, Corresponding Source work available anyone copy, free charge terms License, publicly available network server readily accessible means, must either (1) cause Corresponding Source available, (2) arrange deprive benefit patent license particular work, (3) arrange, manner consistent requirements License, extend patent license downstream recipients. “Knowingly relying” means actual knowledge , patent license, conveying covered work country, recipient’s use covered work country, infringe one identifiable patents country reason believe valid. , pursuant connection single transaction arrangement, convey, propagate procuring conveyance , covered work, grant patent license parties receiving covered work authorizing use, propagate, modify convey specific copy covered work, patent license grant automatically extended recipients covered work works based . patent license “discriminatory” include within scope coverage, prohibits exercise , conditioned non-exercise one rights specifically granted License. may convey covered work party arrangement third party business distributing software, make payment third party based extent activity conveying work, third party grants, parties receive covered work , discriminatory patent license () connection copies covered work conveyed (copies made copies), (b) primarily connection specific products compilations contain covered work, unless entered arrangement, patent license granted, prior 28 March 2007. Nothing License shall construed excluding limiting implied license defenses infringement may otherwise available applicable patent law.","code":""},{"path":"/LICENSE.html","id":"12-no-surrender-of-others-freedom","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"12. No Surrender of Others’ Freedom","title":"GNU General Public License","text":"conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. convey covered work satisfy simultaneously obligations License pertinent obligations, consequence may convey . example, agree terms obligate collect royalty conveying convey Program, way satisfy terms License refrain entirely conveying Program.","code":""},{"path":"/LICENSE.html","id":"13-use-with-the-gnu-affero-general-public-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"13. Use with the GNU Affero General Public License","title":"GNU General Public License","text":"Notwithstanding provision License, permission link combine covered work work licensed version 3 GNU Affero General Public License single combined work, convey resulting work. terms License continue apply part covered work, special requirements GNU Affero General Public License, section 13, concerning interaction network apply combination .","code":""},{"path":"/LICENSE.html","id":"14-revised-versions-of-this-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"14. Revised Versions of this License","title":"GNU General Public License","text":"Free Software Foundation may publish revised /new versions GNU General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies certain numbered version GNU General Public License “later version” applies , option following terms conditions either numbered version later version published Free Software Foundation. Program specify version number GNU General Public License, may choose version ever published Free Software Foundation. Program specifies proxy can decide future versions GNU General Public License can used, proxy’s public statement acceptance version permanently authorizes choose version Program. Later license versions may give additional different permissions. However, additional obligations imposed author copyright holder result choosing follow later version.","code":""},{"path":"/LICENSE.html","id":"15-disclaimer-of-warranty","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"15. Disclaimer of Warranty","title":"GNU General Public License","text":"WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION.","code":""},{"path":"/LICENSE.html","id":"16-limitation-of-liability","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"16. Limitation of Liability","title":"GNU General Public License","text":"EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MODIFIES /CONVEYS PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES.","code":""},{"path":"/LICENSE.html","id":"17-interpretation-of-sections-15-and-16","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"17. Interpretation of Sections 15 and 16","title":"GNU General Public License","text":"disclaimer warranty limitation liability provided given local legal effect according terms, reviewing courts shall apply local law closely approximates absolute waiver civil liability connection Program, unless warranty assumption liability accompanies copy Program return fee. END TERMS CONDITIONS","code":""},{"path":"/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively state exclusion warranty; file least “copyright” line pointer full notice found. Also add information contact electronic paper mail. program terminal interaction, make output short notice like starts interactive mode: hypothetical commands show w show c show appropriate parts General Public License. course, program’s commands might different; GUI interface, use “box”. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. information , apply follow GNU GPL, see <http://www.gnu.org/licenses/>. GNU General Public License permit incorporating program proprietary programs. program subroutine library, may consider useful permit linking proprietary applications library. want , use GNU Lesser General Public License instead License. first, please read <http://www.gnu.org/philosophy/--lgpl.html>.","code":"<one line to give the program's name and a brief idea of what it does.> Copyright (C) <year>  <name of author>  This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.  You should have received a copy of the GNU General Public License along with this program.  If not, see <http://www.gnu.org/licenses/>. <program>  Copyright (C) <year>  <name of author> This program comes with ABSOLUTELY NO WARRANTY; for details type 'show w'. This is free software, and you are welcome to redistribute it under certain conditions; type 'show c' for details."},{"path":"/reference/aggregate_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Aggregate source data — aggregate_data","title":"Aggregate source data — aggregate_data","text":"Aggregates source_data object based field_types() specified load time. Default time period aggregation calendar day","code":""},{"path":"/reference/aggregate_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Aggregate source data — aggregate_data","text":"","code":"aggregate_data(source_data, aggregation_timeunit = \"day\", show_progress = TRUE)"},{"path":"/reference/aggregate_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Aggregate source data — aggregate_data","text":"source_data source_data object returned prepare_data() function aggregation_timeunit Unit time aggregate . Specify one \"day\", \"week\", \"month\", \"quarter\", \"year\". \"week\" option Monday-based. Default = \"day\" show_progress Print progress console. Default = TRUE","code":""},{"path":"/reference/aggregate_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Aggregate source data — aggregate_data","text":"aggregated_data object","code":""},{"path":[]},{"path":"/reference/aggregate_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Aggregate source data — aggregate_data","text":"","code":"# load example data into a data.frame raw_data <- read_data(   system.file(\"extdata\", \"example_prescriptions.csv\", package = \"daiquiri\"),   delim = \",\",   col_names = TRUE )  # validate and prepare the data for aggregation source_data <- prepare_data(   raw_data,   field_types = field_types(     PrescriptionID = ft_uniqueidentifier(),     PrescriptionDate = ft_timepoint(),     AdmissionDate = ft_datetime(includes_time = FALSE),     Drug = ft_freetext(),     Dose = ft_numeric(),     DoseUnit = ft_categorical(),     PatientID = ft_ignore(),     Location = ft_categorical(aggregate_by_each_category = TRUE)   ),   override_column_names = FALSE,   na = c(\"\", \"NULL\") ) #> field_types supplied: #> PrescriptionID\t<field_type_uniqueidentifier> #> PrescriptionDate\t<field_type_timepoint>\toptions: includes_time #> AdmissionDate\t<field_type_datetime> #> Drug\t<field_type_freetext> #> Dose\t<field_type_numeric> #> DoseUnit\t<field_type_categorical> #> PatientID\t<field_type_ignore> #> Location\t<field_type_categorical>\toptions: aggregate_by_each_category #>   #> Checking column names against field_types...  #> Importing source data [NULL]...  #> Checking data against field_types...  #>   Selecting relevant warnings...  #>   Identifying nonconformant values...  #>   Checking and removing missing timepoints...  #> Checking for duplicates...  #>   Sorting data...  #> Loading into source_data structure...  #>   PrescriptionID  #>   PrescriptionDate  #>   AdmissionDate  #>   Drug  #>   Dose  #>   DoseUnit  #>   PatientID  #>   Location  #> Finished   # aggregate the data aggregated_data <- aggregate_data(   source_data,   aggregation_timeunit = \"day\" ) #> Aggregating [] by [day]...  #> Aggregating overall dataset...  #> Aggregating each data_field in turn...  #> 1: PrescriptionID  #> Preparing...  #> Aggregating character field...  #>   By n  #>   By missing_n  #>   By missing_perc  #>   By min_length  #>   By max_length  #>   By mean_length  #> Finished  #> 2: PrescriptionDate  #> Preparing...  #> Aggregating double field...  #>   By n  #>   By midnight_n  #>   By midnight_perc  #> Finished  #> 3: AdmissionDate  #> Preparing...  #> Aggregating double field...  #>   By n  #>   By missing_n  #>   By missing_perc  #>   By nonconformant_n  #>   By nonconformant_perc  #>   By min  #>   By max  #> Finished  #> 4: Drug  #> Preparing...  #> Aggregating character field...  #>   By n  #>   By missing_n  #>   By missing_perc  #> Finished  #> 5: Dose  #> Preparing...  #> Aggregating double field...  #>   By n  #>   By missing_n  #>   By missing_perc  #>   By nonconformant_n  #>   By nonconformant_perc  #>   By min  #>   By max  #>   By mean  #>   By median  #> Finished  #> 6: DoseUnit  #> Preparing...  #> Aggregating character field...  #>   By n  #>   By missing_n  #>   By missing_perc  #>   By distinct  #> Finished  #> 7: Location  #> Preparing...  #> Aggregating character field...  #>   By n  #>   By missing_n  #>   By missing_perc  #>   By distinct  #>   By subcat_n  #>     2 categories found  #>     1: SITE1  #>     2: SITE2  #>   By subcat_perc  #>     2 categories found  #>     1: SITE1  #>     2: SITE2  #> Finished  #> Aggregating calculated fields...  #> [DUPLICATES]:  #> Preparing...  #> Aggregating integer field...  #>   By sum  #>   By nonzero_perc  #> Finished  #> [ALL_FIELDS_COMBINED]:  #> Finished"},{"path":"/reference/availablefieldtypes.html","id":null,"dir":"Reference","previous_headings":"","what":"Available fieldtypes — availablefieldtypes","title":"Available fieldtypes — availablefieldtypes","text":"column source dataset must assigned one fieldtypes, fieldtypes specification.","code":""},{"path":"/reference/availablefieldtypes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Available fieldtypes — availablefieldtypes","text":"","code":"ft_timepoint(includes_time = TRUE, format = \"\")  ft_uniqueidentifier()  ft_categorical(aggregate_by_each_category = FALSE)  ft_numeric()  ft_datetime(includes_time = TRUE, format = \"\")  ft_freetext()  ft_simple()  ft_ignore()"},{"path":"/reference/availablefieldtypes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Available fieldtypes — availablefieldtypes","text":"includes_time TRUE, additional aggregated values generated using time portion (time portion present midnight assumed). FALSE, aggregated values ignore time portion. Default = TRUE format datetime values format `YYYY-MM-DD` `YYYY-MM-DD HH:MM:SS`, alternative format can specified per field level, using readr's col_datetime format specifications, e.g. format = \"%d/%m/%Y\". format supplied, must match complete string. aggregate_by_each_category TRUE, aggregated values generated distinct subcategory well field overall. FALSE, aggregated values generated field overall. Default = FALSE","code":""},{"path":"/reference/availablefieldtypes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Available fieldtypes — availablefieldtypes","text":"ft_timepoint - identifies data field   used independent time variable. one   one specified. ft_uniqueidentifier - identifies data fields   contain (usually computer-generated) identifier entity, e.g.   patient. need unique within dataset. ft_categorical - identifies data fields   treated categorical. ft_numeric - identifies data fields contain numeric values treated continuous. values contain non-numeric characters (including grouping marks) classed non-conformant ft_datetime - identifies data fields contain   date values treated continuous. ft_freetext - identifies data fields contain   free text values. presence/missingness evaluated. ft_simple - identifies data fields   want presence/missingness evaluated (necessarily   free text). ft_ignore - identifies data fields   ignored.  loaded.","code":""},{"path":[]},{"path":"/reference/availablefieldtypes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Available fieldtypes — availablefieldtypes","text":"","code":"fts <- fieldtypes(PatientID = ft_uniqueidentifier(),                             TestID = ft_ignore(),                             TestDate = ft_timepoint(),                             TestName = ft_categorical(aggregate_by_each_category = FALSE),                             TestResult = ft_numeric(),                             ResultDate = ft_datetime(),                             ResultComment = ft_freetext(),                             Location = ft_categorical())"},{"path":"/reference/close_log.html","id":null,"dir":"Reference","previous_headings":"","what":"Closes any active log file — close_log","title":"Closes any active log file — close_log","text":"Closes active log file","code":""},{"path":"/reference/close_log.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Closes any active log file — close_log","text":"","code":"close_log()"},{"path":"/reference/close_log.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Closes any active log file — close_log","text":"log file found, path log file closed, otherwise empty string","code":""},{"path":"/reference/close_log.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Closes any active log file — close_log","text":"","code":"close_log() #> [1] \"\""},{"path":"/reference/create_report.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a data quality report from a data frame — create_report","title":"Create a data quality report from a data frame — create_report","text":"Accepts record-level data data frame, validates expected type content column, generates collection time series plots visual inspection, saves report disk.","code":""},{"path":"/reference/create_report.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a data quality report from a data frame — create_report","text":"","code":"create_report(   df,   field_types,   override_column_names = FALSE,   na = c(\"\", \"NA\", \"NULL\"),   dataset_description = NULL,   aggregation_timeunit = \"day\",   save_directory = \".\",   save_filename = NULL,   show_progress = TRUE,   log_directory = NULL )"},{"path":"/reference/create_report.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a data quality report from a data frame — create_report","text":"df data frame. Rectangular data can read file using read_data(). See Details. field_types field_types() object specifying names types fields (columns) supplied df. See also field_types_available. override_column_names FALSE, column names supplied df must match names specified field_types exactly. TRUE, column names supplied df replaced names specified field_types. specification must therefore contain columns correct order. Default = FALSE na vector containing strings interpreted missing values, Default = c(\"\",\"NA\",\"NULL\"). dataset_description Short description dataset checked. appear report. blank, name data frame object used aggregation_timeunit Unit time aggregate . Specify one \"day\", \"week\", \"month\", \"quarter\", \"year\". \"week\" option Monday-based. Default = \"day\" save_directory String specifying directory save report. Default current directory. save_filename String specifying filename report, excluding file extension. filename supplied, one automatically generated format daiquiri_report_YYMMDD_HHMMSS. show_progress Print progress console. Default = TRUE log_directory String specifying directory save log file. directory supplied, progress logged.","code":""},{"path":"/reference/create_report.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a data quality report from a data frame — create_report","text":"list containing information relating supplied parameters well resulting source_data aggregated_data objects.","code":""},{"path":"/reference/create_report.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a data quality report from a data frame — create_report","text":"order package detect non-conformant values numeric datetime fields, present data frame raw character format. Rectangular data text file automatically read character type use read_data() function. Data frame columns class character still processed according field_types specified.","code":""},{"path":[]},{"path":"/reference/create_report.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a data quality report from a data frame — create_report","text":"","code":"# \\donttest{ # load example data into a data.frame raw_data <- read_data(   system.file(\"extdata\", \"example_prescriptions.csv\", package = \"daiquiri\"),   delim = \",\",   col_names = TRUE )  # create a report in the current directory daiq_obj <- create_report(   raw_data,   field_types = field_types(     PrescriptionID = ft_uniqueidentifier(),     PrescriptionDate = ft_timepoint(),     AdmissionDate = ft_datetime(includes_time = FALSE),     Drug = ft_freetext(),     Dose = ft_numeric(),     DoseUnit = ft_categorical(),     PatientID = ft_ignore(),     Location = ft_categorical(aggregate_by_each_category = TRUE)   ),   override_column_names = FALSE,   na = c(\"\", \"NULL\"),   dataset_description = \"Example data provided with package\",   aggregation_timeunit = \"day\",   save_directory = \".\",   save_filename = \"example_data_report\",   show_progress = TRUE,   log_directory = NULL ) #> field_types supplied: #> PrescriptionID\t<field_type_uniqueidentifier> #> PrescriptionDate\t<field_type_timepoint>\toptions: includes_time #> AdmissionDate\t<field_type_datetime> #> Drug\t<field_type_freetext> #> Dose\t<field_type_numeric> #> DoseUnit\t<field_type_categorical> #> PatientID\t<field_type_ignore> #> Location\t<field_type_categorical>\toptions: aggregate_by_each_category #>   #> Checking column names against field_types...  #> Importing source data [Example data provided with package]...  #> Checking data against field_types...  #>   Selecting relevant warnings...  #>   Identifying nonconformant values...  #>   Checking and removing missing timepoints...  #> Checking for duplicates...  #>   Sorting data...  #> Loading into source_data structure...  #>   PrescriptionID  #>   PrescriptionDate  #>   AdmissionDate  #>   Drug  #>   Dose  #>   DoseUnit  #>   PatientID  #>   Location  #> Finished  #> Aggregating [] by [day]...  #> Aggregating overall dataset...  #> Aggregating each data_field in turn...  #> 1: PrescriptionID  #> Preparing...  #> Aggregating character field...  #>   By n  #>   By missing_n  #>   By missing_perc  #>   By min_length  #>   By max_length  #>   By mean_length  #> Finished  #> 2: PrescriptionDate  #> Preparing...  #> Aggregating double field...  #>   By n  #>   By midnight_n  #>   By midnight_perc  #> Finished  #> 3: AdmissionDate  #> Preparing...  #> Aggregating double field...  #>   By n  #>   By missing_n  #>   By missing_perc  #>   By nonconformant_n  #>   By nonconformant_perc  #>   By min  #>   By max  #> Finished  #> 4: Drug  #> Preparing...  #> Aggregating character field...  #>   By n  #>   By missing_n  #>   By missing_perc  #> Finished  #> 5: Dose  #> Preparing...  #> Aggregating double field...  #>   By n  #>   By missing_n  #>   By missing_perc  #>   By nonconformant_n  #>   By nonconformant_perc  #>   By min  #>   By max  #>   By mean  #>   By median  #> Finished  #> 6: DoseUnit  #> Preparing...  #> Aggregating character field...  #>   By n  #>   By missing_n  #>   By missing_perc  #>   By distinct  #> Finished  #> 7: Location  #> Preparing...  #> Aggregating character field...  #>   By n  #>   By missing_n  #>   By missing_perc  #>   By distinct  #>   By subcat_n  #>     2 categories found  #>     1: SITE1  #>     2: SITE2  #>   By subcat_perc  #>     2 categories found  #>     1: SITE1  #>     2: SITE2  #> Finished  #> Aggregating calculated fields...  #> [DUPLICATES]:  #> Preparing...  #> Aggregating integer field...  #>   By sum  #>   By nonzero_perc  #> Finished  #> [ALL_FIELDS_COMBINED]:  #> Finished  #> Generating html report...  #>  #>  #> processing file: report_htmldoc.Rmd #>    |                                                                               |                                                                      |   0%   |                                                                               |..                                                                    |   3% #>   ordinary text without R code #>  #>    |                                                                               |.....                                                                 |   7% #> label: setup (with options)  #> List of 1 #>  $ include: logi FALSE #>  #>    |                                                                               |.......                                                               |  10% #>   ordinary text without R code #>  #>    |                                                                               |..........                                                            |  14% #> label: unnamed-chunk-1 (with options)  #> List of 2 #>  $ echo  : logi FALSE #>  $ engine: chr \"css\" #>  #>    |                                                                               |............                                                          |  17% #>    inline R code fragments #>  #>    |                                                                               |..............                                                        |  21% #> label: source_data #>    |                                                                               |.................                                                     |  24% #>   ordinary text without R code #>  #>    |                                                                               |...................                                                   |  28% #> label: fields-imported #>    |                                                                               |......................                                                |  31% #>   ordinary text without R code #>  #>    |                                                                               |........................                                              |  34% #> label: fields-ignored #>    |                                                                               |...........................                                           |  38% #>   ordinary text without R code #>  #>    |                                                                               |.............................                                         |  41% #> label: validation-warnings #>    |                                                                               |...............................                                       |  45% #>   ordinary text without R code #>  #>    |                                                                               |..................................                                    |  48% #> label: source_data-summary #>    |                                                                               |....................................                                  |  52% #>   ordinary text without R code #>  #>    |                                                                               |.......................................                               |  55% #> label: aggregated_data #>    |                                                                               |.........................................                             |  59% #>   ordinary text without R code #>  #>    |                                                                               |...........................................                           |  62% #> label: overview-presence #>    |                                                                               |..............................................                        |  66% #>   ordinary text without R code #>  #>    |                                                                               |................................................                      |  69% #> label: overview-missing #>    |                                                                               |...................................................                   |  72% #>   ordinary text without R code #>  #>    |                                                                               |.....................................................                 |  76% #> label: overview-nonconformant #>    |                                                                               |........................................................              |  79% #>   ordinary text without R code #>  #>    |                                                                               |..........................................................            |  83% #> label: overview-duplicates #>    |                                                                               |............................................................          |  86% #>   ordinary text without R code #>  #>    |                                                                               |...............................................................       |  90% #> label: aggregated_data-summary #>    |                                                                               |.................................................................     |  93% #>   ordinary text without R code #>  #>    |                                                                               |....................................................................  |  97% #> label: individual-fields (with options)  #> List of 1 #>  $ results: chr \"asis\" #>  #>    |                                                                               |......................................................................| 100% #>   ordinary text without R code #>  #>  #> output file: report_htmldoc.knit.md #> \"C:/Program Files/RStudio/bin/quarto/bin/pandoc\" +RTS -K512m -RTS report_htmldoc.knit.md --to html4 --from markdown+autolink_bare_uris+tex_math_single_backslash --output pandoc47442b7f6268.html --lua-filter \"C:\\Users\\pquan\\Documents\\Rworkspace\\library\\rmarkdown\\rmarkdown\\lua\\pagebreak.lua\" --lua-filter \"C:\\Users\\pquan\\Documents\\Rworkspace\\library\\rmarkdown\\rmarkdown\\lua\\latex-div.lua\" --self-contained --variable bs3=TRUE --standalone --section-divs --template \"C:\\Users\\pquan\\Documents\\Rworkspace\\library\\rmarkdown\\rmd\\h\\default.html\" --no-highlight --variable highlightjs=1 --variable theme=bootstrap --mathjax --variable \"mathjax-url=https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\" --include-in-header \"C:\\Users\\pquan\\AppData\\Local\\Temp\\RtmpQ3Y4Sq\\rmarkdown-str474416e63525.html\"  #>  #> Output created: example_data_report.html #> Report saved to: ./example_data_report.html  file.remove(\"./example_data_report.html\") #> [1] TRUE # }"},{"path":"/reference/daiquiri-package.html","id":null,"dir":"Reference","previous_headings":"","what":"daiquiri: Data Quality Reporting for Temporal Datasets — daiquiri-package","title":"daiquiri: Data Quality Reporting for Temporal Datasets — daiquiri-package","text":"Generate reports enable quick visual review temporal shifts record-level data. Time series plots showing aggregated values automatically created data field (column) depending contents (e.g. min/max/mean values numeric data, . distinct values categorical data), well overviews missing values, non-conformant values, duplicated rows. resulting reports sharable can contribute forming transparent record entire analysis process. designed Electronic Health Records mind, can used type record-level temporal data (.e. tabular data row represents single “event”, one column contains \"event date\", columns contain associated values event).","code":""},{"path":[]},{"path":"/reference/daiquiri-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"daiquiri: Data Quality Reporting for Temporal Datasets — daiquiri-package","text":"Maintainer: T. Phuong Quan phuong.quan@ndm.ox.ac.uk (ORCID) contributors: Jack Cregan [contributor] University Oxford [copyright holder] National Institute Health Research (NIHR) [funder]","code":""},{"path":"/reference/daiquiri.html","id":null,"dir":"Reference","previous_headings":"","what":"Data quality reporting for temporal datasets — daiquiri","title":"Data quality reporting for temporal datasets — daiquiri","text":"Generate reports enable quick visual review temporal shifts record-level data. Time series plots showing aggregated values automatically created data field (column) depending contents (e.g. min/max/mean values numeric data, . distinct values categorical data), well overviews missing values, non-conformant values, duplicated rows. resulting reports sharable can contribute forming transparent record entire analysis process. designed electronic health records mind, can used type record-level temporal data (.e. tabular data row represents single “event”, one column contains \"event date\", columns contain associated values event).","code":""},{"path":"/reference/daiquiri.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Data quality reporting for temporal datasets — daiquiri","text":"Classes S3 best place start create_report function, walkthrough vignette: vignette(\"daiquiri\", package = \"daiquiri\").","code":""},{"path":"/reference/export_aggregated_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Export aggregated data — export_aggregated_data","title":"Export aggregated data — export_aggregated_data","text":"Export aggregated data disk.  Creates separate file aggregated field dataset.","code":""},{"path":"/reference/export_aggregated_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Export aggregated data — export_aggregated_data","text":"","code":"export_aggregated_data(   aggregated_data,   save_directory,   save_file_prefix = \"\",   save_file_type = \"csv\" )"},{"path":"/reference/export_aggregated_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Export aggregated data — export_aggregated_data","text":"aggregated_data aggregated_data object save_directory String. Full relative path save folder save_file_prefix String. Optional prefix exported filenames save_file_type String. Filetype extension supported readr, currently csv allowed","code":""},{"path":"/reference/export_aggregated_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Export aggregated data — export_aggregated_data","text":"(invisibly) aggregated_data object passed ","code":""},{"path":"/reference/export_aggregated_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Export aggregated data — export_aggregated_data","text":"","code":"raw_data <- read_data(   system.file(\"extdata\", \"example_prescriptions.csv\", package = \"daiquiri\"),   delim = \",\",   col_names = TRUE )  source_data <- prepare_data(   raw_data,   field_types = field_types(     PrescriptionID = ft_uniqueidentifier(),     PrescriptionDate = ft_timepoint(),     AdmissionDate = ft_datetime(includes_time = FALSE),     Drug = ft_freetext(),     Dose = ft_numeric(),     DoseUnit = ft_categorical(),     PatientID = ft_ignore(),     Location = ft_categorical(aggregate_by_each_category = TRUE)   ),   override_column_names = FALSE,   na = c(\"\", \"NULL\") ) #> field_types supplied: #> PrescriptionID\t<field_type_uniqueidentifier> #> PrescriptionDate\t<field_type_timepoint>\toptions: includes_time #> AdmissionDate\t<field_type_datetime> #> Drug\t<field_type_freetext> #> Dose\t<field_type_numeric> #> DoseUnit\t<field_type_categorical> #> PatientID\t<field_type_ignore> #> Location\t<field_type_categorical>\toptions: aggregate_by_each_category #>   #> Checking column names against field_types...  #> Importing source data [NULL]...  #> Checking data against field_types...  #>   Selecting relevant warnings...  #>   Identifying nonconformant values...  #>   Checking and removing missing timepoints...  #> Checking for duplicates...  #>   Sorting data...  #> Loading into source_data structure...  #>   PrescriptionID  #>   PrescriptionDate  #>   AdmissionDate  #>   Drug  #>   Dose  #>   DoseUnit  #>   PatientID  #>   Location  #> Finished   aggregated_data <- aggregate_data(   source_data,   aggregation_timeunit = \"day\" ) #> Aggregating [] by [day]...  #> Aggregating overall dataset...  #> Aggregating each data_field in turn...  #> 1: PrescriptionID  #> Preparing...  #> Aggregating character field...  #>   By n  #>   By missing_n  #>   By missing_perc  #>   By min_length  #>   By max_length  #>   By mean_length  #> Finished  #> 2: PrescriptionDate  #> Preparing...  #> Aggregating double field...  #>   By n  #>   By midnight_n  #>   By midnight_perc  #> Finished  #> 3: AdmissionDate  #> Preparing...  #> Aggregating double field...  #>   By n  #>   By missing_n  #>   By missing_perc  #>   By nonconformant_n  #>   By nonconformant_perc  #>   By min  #>   By max  #> Finished  #> 4: Drug  #> Preparing...  #> Aggregating character field...  #>   By n  #>   By missing_n  #>   By missing_perc  #> Finished  #> 5: Dose  #> Preparing...  #> Aggregating double field...  #>   By n  #>   By missing_n  #>   By missing_perc  #>   By nonconformant_n  #>   By nonconformant_perc  #>   By min  #>   By max  #>   By mean  #>   By median  #> Finished  #> 6: DoseUnit  #> Preparing...  #> Aggregating character field...  #>   By n  #>   By missing_n  #>   By missing_perc  #>   By distinct  #> Finished  #> 7: Location  #> Preparing...  #> Aggregating character field...  #>   By n  #>   By missing_n  #>   By missing_perc  #>   By distinct  #>   By subcat_n  #>     2 categories found  #>     1: SITE1  #>     2: SITE2  #>   By subcat_perc  #>     2 categories found  #>     1: SITE1  #>     2: SITE2  #> Finished  #> Aggregating calculated fields...  #> [DUPLICATES]:  #> Preparing...  #> Aggregating integer field...  #>   By sum  #>   By nonzero_perc  #> Finished  #> [ALL_FIELDS_COMBINED]:  #> Finished   export_aggregated_data(   aggregated_data,   save_directory = \".\",   save_file_prefix = \"ex_\" )  # \\dontshow{ f <- list.files(\".\", \"^ex_.*csv$\") file.remove(f) #> [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE # }"},{"path":"/reference/fieldtypes.html","id":null,"dir":"Reference","previous_headings":"","what":"Create fieldtypes specification — fieldtypes","title":"Create fieldtypes specification — fieldtypes","text":"Specify names types fields source data frame. important data field aggregated different ways, depending fieldtype.  See availablefieldtypes","code":""},{"path":"/reference/fieldtypes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create fieldtypes specification — fieldtypes","text":"","code":"fieldtypes(...)"},{"path":"/reference/fieldtypes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create fieldtypes specification — fieldtypes","text":"... names types fields (columns) source data.","code":""},{"path":"/reference/fieldtypes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create fieldtypes specification — fieldtypes","text":"fieldtypes object","code":""},{"path":[]},{"path":"/reference/fieldtypes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create fieldtypes specification — fieldtypes","text":"","code":"fts <- fieldtypes(PatientID = ft_uniqueidentifier(),                             TestID = ft_ignore(),                             TestDate = ft_timepoint(),                             TestName = ft_categorical(aggregate_by_each_category = FALSE),                             TestResult = ft_numeric(),                             ResultDate = ft_datetime(),                             ResultComment = ft_freetext(),                             Location = ft_categorical())"},{"path":"/reference/fieldtypes_template.html","id":null,"dir":"Reference","previous_headings":"","what":"Print a template fieldtypes specification to console — fieldtypes_template","title":"Print a template fieldtypes specification to console — fieldtypes_template","text":"Helper function generate template code fieldtypes() specification, based supplied data frame. fields (columns) specification defined using default_fieldtype, console output can copied edited used input create_report() prepare_data().","code":""},{"path":"/reference/fieldtypes_template.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print a template fieldtypes specification to console — fieldtypes_template","text":"","code":"fieldtypes_template(df, default_fieldtype = ft_ignore())"},{"path":"/reference/fieldtypes_template.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print a template fieldtypes specification to console — fieldtypes_template","text":"df data frame including column names template specification default_fieldtype fieldtype used column. Default = ft_ignore(). See  availablefieldtypes","code":""},{"path":[]},{"path":"/reference/fieldtypes_template.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print a template fieldtypes specification to console — fieldtypes_template","text":"","code":"df <- data.frame(col1 = rep(\"2022-01-01\", 5), col2 = rep(1, 5), col3 = 1:5, col4 = rnorm(5))  fieldtypes_template(df, default_fieldtype = ft_numeric()) #> fieldtypes( \"col1\" = ft_numeric(), #> \t\"col2\" = ft_numeric(), #> \t\"col3\" = ft_numeric(), #> \t\"col4\" = ft_numeric() )"},{"path":"/reference/field_types.html","id":null,"dir":"Reference","previous_headings":"","what":"Create field_types specification — field_types","title":"Create field_types specification — field_types","text":"Specify names types fields source data frame. important data field aggregated different ways, depending field_type.  See field_types_available","code":""},{"path":"/reference/field_types.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create field_types specification — field_types","text":"","code":"field_types(...)"},{"path":"/reference/field_types.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create field_types specification — field_types","text":"... names types fields (columns) source data.","code":""},{"path":"/reference/field_types.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create field_types specification — field_types","text":"field_types object","code":""},{"path":[]},{"path":"/reference/field_types.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create field_types specification — field_types","text":"","code":"fts <- field_types(   PatientID = ft_uniqueidentifier(),   TestID = ft_ignore(),   TestDate = ft_timepoint(),   TestName = ft_categorical(aggregate_by_each_category = FALSE),   TestResult = ft_numeric(),   ResultDate = ft_datetime(),   ResultComment = ft_freetext(),   Location = ft_categorical() )"},{"path":"/reference/field_types_available.html","id":null,"dir":"Reference","previous_headings":"","what":"Types of data fields available for specification — field_types_available","title":"Types of data fields available for specification — field_types_available","text":"column source dataset must assigned particular ft_xx depending type data contains. done field_types() specification.","code":""},{"path":"/reference/field_types_available.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Types of data fields available for specification — field_types_available","text":"","code":"ft_timepoint(includes_time = TRUE, format = \"\")  ft_uniqueidentifier()  ft_categorical(aggregate_by_each_category = FALSE)  ft_numeric()  ft_datetime(includes_time = TRUE, format = \"\")  ft_freetext()  ft_simple()  ft_ignore()"},{"path":"/reference/field_types_available.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Types of data fields available for specification — field_types_available","text":"includes_time TRUE, additional aggregated values generated using time portion (time portion present midnight assumed). FALSE, aggregated values ignore time portion. Default = TRUE format datetime values format YYYY-MM-DD YYYY-MM-DD HH:MM:SS, alternative format can specified per field level, using readr::col_datetime() format specifications, e.g. format = \"%d/%m/%Y\". format supplied, must match complete string. aggregate_by_each_category TRUE, aggregated values generated distinct subcategory well field overall. FALSE, aggregated values generated field overall. Default = FALSE","code":""},{"path":"/reference/field_types_available.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Types of data fields available for specification — field_types_available","text":"field_type object denoting type data column","code":""},{"path":"/reference/field_types_available.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Types of data fields available for specification — field_types_available","text":"ft_timepoint() - identifies data field used independent time variable. one one specified. ft_uniqueidentifier() - identifies data fields contain (usually computer-generated) identifier entity, e.g. patient. need unique within dataset. ft_categorical() - identifies data fields treated categorical. ft_numeric() - identifies data fields contain numeric values treated continuous. values contain non-numeric characters (including grouping marks) classed non-conformant ft_datetime() - identifies data fields contain date values treated continuous. ft_freetext() - identifies data fields contain free text values. presence/missingness evaluated. ft_simple() - identifies data fields want presence/missingness evaluated (necessarily free text). ft_ignore() - identifies data fields ignored.  loaded.","code":""},{"path":[]},{"path":"/reference/field_types_available.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Types of data fields available for specification — field_types_available","text":"","code":"fts <- field_types(   PatientID = ft_uniqueidentifier(),   TestID = ft_ignore(),   TestDate = ft_timepoint(),   TestName = ft_categorical(aggregate_by_each_category = FALSE),   TestResult = ft_numeric(),   ResultDate = ft_datetime(),   ResultComment = ft_freetext(),   Location = ft_categorical() )"},{"path":"/reference/initialise_log.html","id":null,"dir":"Reference","previous_headings":"","what":"Initialise log file — initialise_log","title":"Initialise log file — initialise_log","text":"Choose directory save log file. called, log file created.","code":""},{"path":"/reference/initialise_log.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Initialise log file — initialise_log","text":"","code":"initialise_log(log_directory)"},{"path":"/reference/initialise_log.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Initialise log file — initialise_log","text":"log_directory String containing directory save log file","code":""},{"path":"/reference/initialise_log.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Initialise log file — initialise_log","text":"Character string containing full path newly-created log file","code":""},{"path":"/reference/initialise_log.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Initialise log file — initialise_log","text":"","code":"log_name <- initialise_log(\".\") # \\dontshow{ close_log() #> [1] \"C:\\\\Users\\\\pquan\\\\Documents\\\\Rworkspace\\\\daiquiri\\\\docs\\\\reference\\\\daiquiri_20221014_115835.log\" file.remove(log_name) #> [1] TRUE # }"},{"path":"/reference/log_close.html","id":null,"dir":"Reference","previous_headings":"","what":"Closes any active log file — log_close","title":"Closes any active log file — log_close","text":"Closes active log file","code":""},{"path":"/reference/log_close.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Closes any active log file — log_close","text":"","code":"log_close()"},{"path":"/reference/log_close.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Closes any active log file — log_close","text":"","code":"log_close()"},{"path":"/reference/log_initialise.html","id":null,"dir":"Reference","previous_headings":"","what":"Initialise log file — log_initialise","title":"Initialise log file — log_initialise","text":"Choose directory save log file. called, log file created.","code":""},{"path":"/reference/log_initialise.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Initialise log file — log_initialise","text":"","code":"log_initialise(log_directory)"},{"path":"/reference/log_initialise.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Initialise log file — log_initialise","text":"log_directory String containing directory save log file","code":""},{"path":"/reference/log_initialise.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Initialise log file — log_initialise","text":"Character string containing full path newly-created log file","code":""},{"path":"/reference/log_initialise.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Initialise log file — log_initialise","text":"","code":"logname <- log_initialise(\".\")    # \\dontshow{   log_close()   file.remove(logname) #> [1] TRUE   # }"},{"path":"/reference/prepare_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare source data — prepare_data","title":"Prepare source data — prepare_data","text":"Validate data frame field_types() specification, prepare aggregation.","code":""},{"path":"/reference/prepare_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare source data — prepare_data","text":"","code":"prepare_data(   df,   field_types,   override_column_names = FALSE,   na = c(\"\", \"NA\", \"NULL\"),   dataset_description = NULL,   show_progress = TRUE )"},{"path":"/reference/prepare_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare source data — prepare_data","text":"df data frame field_types field_types() object specifying names types fields (columns) supplied df. See also field_types_available. override_column_names FALSE, column names supplied df must match names specified field_types exactly. TRUE, column names supplied df replaced names specified field_types. specification must therefore contain columns correct order. Default = FALSE na vector containing strings interpreted missing values, Default = c(\"\",\"NA\",\"NULL\"). dataset_description Short description dataset checked. appear report. blank, name data frame object used show_progress Print progress console. Default = TRUE","code":""},{"path":"/reference/prepare_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare source data — prepare_data","text":"source_data object","code":""},{"path":[]},{"path":"/reference/prepare_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare source data — prepare_data","text":"","code":"# load example data into a data.frame raw_data <- read_data(   system.file(\"extdata\", \"example_prescriptions.csv\", package = \"daiquiri\"),   delim = \",\",   col_names = TRUE )  # validate and prepare the data for aggregation source_data <- prepare_data(   raw_data,   field_types = field_types(     PrescriptionID = ft_uniqueidentifier(),     PrescriptionDate = ft_timepoint(),     AdmissionDate = ft_datetime(includes_time = FALSE),     Drug = ft_freetext(),     Dose = ft_numeric(),     DoseUnit = ft_categorical(),     PatientID = ft_ignore(),     Location = ft_categorical(aggregate_by_each_category = TRUE)   ),   override_column_names = FALSE,   na = c(\"\", \"NULL\"),   dataset_description = \"Example data provided with package\" ) #> field_types supplied: #> PrescriptionID\t<field_type_uniqueidentifier> #> PrescriptionDate\t<field_type_timepoint>\toptions: includes_time #> AdmissionDate\t<field_type_datetime> #> Drug\t<field_type_freetext> #> Dose\t<field_type_numeric> #> DoseUnit\t<field_type_categorical> #> PatientID\t<field_type_ignore> #> Location\t<field_type_categorical>\toptions: aggregate_by_each_category #>   #> Checking column names against field_types...  #> Importing source data [Example data provided with package]...  #> Checking data against field_types...  #>   Selecting relevant warnings...  #>   Identifying nonconformant values...  #>   Checking and removing missing timepoints...  #> Checking for duplicates...  #>   Sorting data...  #> Loading into source_data structure...  #>   PrescriptionID  #>   PrescriptionDate  #>   AdmissionDate  #>   Drug  #>   Dose  #>   DoseUnit  #>   PatientID  #>   Location  #> Finished"},{"path":"/reference/print_field_types_template.html","id":null,"dir":"Reference","previous_headings":"","what":"Print a template field_types() specification to console — print_field_types_template","title":"Print a template field_types() specification to console — print_field_types_template","text":"Helper function generate template code field_types() specification, based supplied data frame. fields (columns) specification defined using default_field_type, console output can copied edited used input create_report() prepare_data().","code":""},{"path":"/reference/print_field_types_template.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print a template field_types() specification to console — print_field_types_template","text":"","code":"print_field_types_template(df, default_field_type = ft_ignore())"},{"path":"/reference/print_field_types_template.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print a template field_types() specification to console — print_field_types_template","text":"df data frame including column names template specification default_field_type field_type used column. Default = ft_ignore(). See  field_types_available()","code":""},{"path":"/reference/print_field_types_template.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print a template field_types() specification to console — print_field_types_template","text":"(invisibly) Character string containing template code","code":""},{"path":[]},{"path":"/reference/print_field_types_template.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print a template field_types() specification to console — print_field_types_template","text":"","code":"df <- data.frame(col1 = rep(\"2022-01-01\", 5), col2 = rep(1, 5), col3 = 1:5, col4 = rnorm(5))  print_field_types_template(df, default_field_type = ft_numeric()) #> field_types( \"col1\" = ft_numeric(), #> \t\"col2\" = ft_numeric(), #> \t\"col3\" = ft_numeric(), #> \t\"col4\" = ft_numeric() )"},{"path":"/reference/read_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Read data from delimited file without doing any datatype conversion — read_data","title":"Read data from delimited file without doing any datatype conversion — read_data","text":"Read rectangular data delimited file, columns read character type package can later detect non-conformant values. Operates restricted implementation readr::read_delim().","code":""},{"path":"/reference/read_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read data from delimited file without doing any datatype conversion — read_data","text":"","code":"read_data(   file,   delim = NULL,   col_names = TRUE,   quote = \"\\\"\",   trim_ws = TRUE,   comment = \"\",   skip = 0,   n_max = Inf,   show_progress = TRUE )"},{"path":"/reference/read_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read data from delimited file without doing any datatype conversion — read_data","text":"file string containing path file containing data load, URL starting http://, file://, etc. Compressed files extension .gz, .bz2, .xz .zip supported. delim Single character used separate fields within record. E.g. \",\" \"\\t\" col_names Either TRUE, FALSE character vector column names. TRUE, first row input used column names, included data frame. FALSE, column names generated automatically. Default = TRUE quote Single character used quote strings. trim_ws leading trailing whitespace trimmed field? comment string used identify comments. text comment characters silently ignored skip Number lines skip reading data. comment supplied commented lines ignored skipping n_max Maximum number lines read. show_progress Display progress bar? Default = TRUE","code":""},{"path":"/reference/read_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read data from delimited file without doing any datatype conversion — read_data","text":"data frame","code":""},{"path":[]},{"path":"/reference/read_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Read data from delimited file without doing any datatype conversion — read_data","text":"","code":"raw_data <- read_data(   system.file(\"extdata\", \"example_prescriptions.csv\", package = \"daiquiri\"),   delim = \",\",   col_names = TRUE )"},{"path":"/reference/report_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate report from existing objects — report_data","title":"Generate report from existing objects — report_data","text":"Generate report previously-created source_data aggregated_data objects","code":""},{"path":"/reference/report_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate report from existing objects — report_data","text":"","code":"report_data(   source_data,   aggregated_data,   save_directory = \".\",   save_filename = NULL,   format = \"html\",   show_progress = TRUE )"},{"path":"/reference/report_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate report from existing objects — report_data","text":"source_data source_data object returned prepare_data() function aggregated_data aggregated_data object returned aggregate_data() function save_directory String specifying directory save report. Default current directory. save_filename String specifying filename report, excluding file extension. filename supplied, one automatically generated format daiquiri_report_YYMMDD_HHMMSS. format File format report. Currently \"html\" supported show_progress Print progress console. Default = TRUE","code":""},{"path":"/reference/report_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate report from existing objects — report_data","text":"string containing name path saved report","code":""},{"path":[]},{"path":"/reference/report_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate report from existing objects — report_data","text":"","code":"# \\donttest{ # load example data into a data.frame raw_data <- read_data(   system.file(\"extdata\", \"example_prescriptions.csv\", package = \"daiquiri\"),   delim = \",\",   col_names = TRUE )  # validate and prepare the data for aggregation source_data <- prepare_data(   raw_data,   field_types = field_types(     PrescriptionID = ft_uniqueidentifier(),     PrescriptionDate = ft_timepoint(),     AdmissionDate = ft_datetime(includes_time = FALSE),     Drug = ft_freetext(),     Dose = ft_numeric(),     DoseUnit = ft_categorical(),     PatientID = ft_ignore(),     Location = ft_categorical(aggregate_by_each_category = TRUE)   ),   override_column_names = FALSE,   na = c(\"\", \"NULL\"),   dataset_description = \"Example data provided with package\",   show_progress = TRUE ) #> field_types supplied: #> PrescriptionID\t<field_type_uniqueidentifier> #> PrescriptionDate\t<field_type_timepoint>\toptions: includes_time #> AdmissionDate\t<field_type_datetime> #> Drug\t<field_type_freetext> #> Dose\t<field_type_numeric> #> DoseUnit\t<field_type_categorical> #> PatientID\t<field_type_ignore> #> Location\t<field_type_categorical>\toptions: aggregate_by_each_category #>   #> Checking column names against field_types...  #> Importing source data [Example data provided with package]...  #> Checking data against field_types...  #>   Selecting relevant warnings...  #>   Identifying nonconformant values...  #>   Checking and removing missing timepoints...  #> Checking for duplicates...  #>   Sorting data...  #> Loading into source_data structure...  #>   PrescriptionID  #>   PrescriptionDate  #>   AdmissionDate  #>   Drug  #>   Dose  #>   DoseUnit  #>   PatientID  #>   Location  #> Finished   # aggregate the data aggregated_data <- aggregate_data(   source_data,   aggregation_timeunit = \"day\",   show_progress = TRUE ) #> Aggregating [] by [day]...  #> Aggregating overall dataset...  #> Aggregating each data_field in turn...  #> 1: PrescriptionID  #> Preparing...  #> Aggregating character field...  #>   By n  #>   By missing_n  #>   By missing_perc  #>   By min_length  #>   By max_length  #>   By mean_length  #> Finished  #> 2: PrescriptionDate  #> Preparing...  #> Aggregating double field...  #>   By n  #>   By midnight_n  #>   By midnight_perc  #> Finished  #> 3: AdmissionDate  #> Preparing...  #> Aggregating double field...  #>   By n  #>   By missing_n  #>   By missing_perc  #>   By nonconformant_n  #>   By nonconformant_perc  #>   By min  #>   By max  #> Finished  #> 4: Drug  #> Preparing...  #> Aggregating character field...  #>   By n  #>   By missing_n  #>   By missing_perc  #> Finished  #> 5: Dose  #> Preparing...  #> Aggregating double field...  #>   By n  #>   By missing_n  #>   By missing_perc  #>   By nonconformant_n  #>   By nonconformant_perc  #>   By min  #>   By max  #>   By mean  #>   By median  #> Finished  #> 6: DoseUnit  #> Preparing...  #> Aggregating character field...  #>   By n  #>   By missing_n  #>   By missing_perc  #>   By distinct  #> Finished  #> 7: Location  #> Preparing...  #> Aggregating character field...  #>   By n  #>   By missing_n  #>   By missing_perc  #>   By distinct  #>   By subcat_n  #>     2 categories found  #>     1: SITE1  #>     2: SITE2  #>   By subcat_perc  #>     2 categories found  #>     1: SITE1  #>     2: SITE2  #> Finished  #> Aggregating calculated fields...  #> [DUPLICATES]:  #> Preparing...  #> Aggregating integer field...  #>   By sum  #>   By nonzero_perc  #> Finished  #> [ALL_FIELDS_COMBINED]:  #> Finished   # save a report in the current directory using the previously-created objects report_data(   source_data,   aggregated_data,   save_directory = \".\",   save_filename = \"example_data_report\",   show_progress = TRUE ) #> Generating html report...  #>  #>  #> processing file: report_htmldoc.Rmd #>    |                                                                               |                                                                      |   0%   |                                                                               |..                                                                    |   3% #>   ordinary text without R code #>  #>    |                                                                               |.....                                                                 |   7% #> label: setup (with options)  #> List of 1 #>  $ include: logi FALSE #>  #>    |                                                                               |.......                                                               |  10% #>   ordinary text without R code #>  #>    |                                                                               |..........                                                            |  14% #> label: unnamed-chunk-1 (with options)  #> List of 2 #>  $ echo  : logi FALSE #>  $ engine: chr \"css\" #>  #>    |                                                                               |............                                                          |  17% #>    inline R code fragments #>  #>    |                                                                               |..............                                                        |  21% #> label: source_data #>    |                                                                               |.................                                                     |  24% #>   ordinary text without R code #>  #>    |                                                                               |...................                                                   |  28% #> label: fields-imported #>    |                                                                               |......................                                                |  31% #>   ordinary text without R code #>  #>    |                                                                               |........................                                              |  34% #> label: fields-ignored #>    |                                                                               |...........................                                           |  38% #>   ordinary text without R code #>  #>    |                                                                               |.............................                                         |  41% #> label: validation-warnings #>    |                                                                               |...............................                                       |  45% #>   ordinary text without R code #>  #>    |                                                                               |..................................                                    |  48% #> label: source_data-summary #>    |                                                                               |....................................                                  |  52% #>   ordinary text without R code #>  #>    |                                                                               |.......................................                               |  55% #> label: aggregated_data #>    |                                                                               |.........................................                             |  59% #>   ordinary text without R code #>  #>    |                                                                               |...........................................                           |  62% #> label: overview-presence #>    |                                                                               |..............................................                        |  66% #>   ordinary text without R code #>  #>    |                                                                               |................................................                      |  69% #> label: overview-missing #>    |                                                                               |...................................................                   |  72% #>   ordinary text without R code #>  #>    |                                                                               |.....................................................                 |  76% #> label: overview-nonconformant #>    |                                                                               |........................................................              |  79% #>   ordinary text without R code #>  #>    |                                                                               |..........................................................            |  83% #> label: overview-duplicates #>    |                                                                               |............................................................          |  86% #>   ordinary text without R code #>  #>    |                                                                               |...............................................................       |  90% #> label: aggregated_data-summary #>    |                                                                               |.................................................................     |  93% #>   ordinary text without R code #>  #>    |                                                                               |....................................................................  |  97% #> label: individual-fields (with options)  #> List of 1 #>  $ results: chr \"asis\" #>  #>    |                                                                               |......................................................................| 100% #>   ordinary text without R code #>  #>  #> output file: report_htmldoc.knit.md #> \"C:/Program Files/RStudio/bin/quarto/bin/pandoc\" +RTS -K512m -RTS report_htmldoc.knit.md --to html4 --from markdown+autolink_bare_uris+tex_math_single_backslash --output pandoc474411ae19ef.html --lua-filter \"C:\\Users\\pquan\\Documents\\Rworkspace\\library\\rmarkdown\\rmarkdown\\lua\\pagebreak.lua\" --lua-filter \"C:\\Users\\pquan\\Documents\\Rworkspace\\library\\rmarkdown\\rmarkdown\\lua\\latex-div.lua\" --self-contained --variable bs3=TRUE --standalone --section-divs --template \"C:\\Users\\pquan\\Documents\\Rworkspace\\library\\rmarkdown\\rmd\\h\\default.html\" --no-highlight --variable highlightjs=1 --variable theme=bootstrap --mathjax --variable \"mathjax-url=https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\" --include-in-header \"C:\\Users\\pquan\\AppData\\Local\\Temp\\RtmpQ3Y4Sq\\rmarkdown-str47448b75097.html\"  #>  #> Output created: example_data_report.html #> Report saved to: ./example_data_report.html  #> [1] \"./example_data_report.html\" file.remove(\"./example_data_report.html\") #> [1] TRUE # }"},{"path":"/news/index.html","id":"daiquiri-development-version","dir":"Changelog","previous_headings":"","what":"daiquiri (development version)","title":"daiquiri (development version)","text":"lot breaking changes objects renamed better consistency style","code":""},{"path":"/news/index.html","id":"breaking-changes-development-version","dir":"Changelog","previous_headings":"","what":"Breaking changes","title":"daiquiri (development version)","text":"field_types() replaces fieldtypes(). create_report(), prepare_data(), aggregate_data(), report_data() parameters renamed. parameters renamed. initialise_log() replaces log_initialise(). close_log() replaces log_close(). print_field_types_template() replaces field_types_template()","code":""},{"path":"/news/index.html","id":"bug-fixes-and-minor-improvements-development-version","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"daiquiri (development version)","text":"Fixed error user passes data.table (create_report() prepare_data()) contains non-character columns. close_log() now returns path closed log file (). example_prescriptions replaces example_dataset example dataset supplied package","code":""},{"path":"/news/index.html","id":"daiquiri-070-2022-04-20","dir":"Changelog","previous_headings":"","what":"daiquiri 0.7.0 (2022-04-20)","title":"daiquiri 0.7.0 (2022-04-20)","text":"release moves reading csv files separate function order make configurable handle parsing fields character data user.","code":""},{"path":"/news/index.html","id":"breaking-changes-0-7-0","dir":"Changelog","previous_headings":"","what":"Breaking changes","title":"daiquiri 0.7.0 (2022-04-20)","text":"create_report() now accepts dataframe first parameter. textfile_contains_column_names parameter removed. load_data() replaced read_data() prepare_data(). log_initialise() function: dirpath parameter renamed log_directory.","code":""},{"path":"/news/index.html","id":"new-features-0-7-0","dir":"Changelog","previous_headings":"","what":"New features","title":"daiquiri 0.7.0 (2022-04-20)","text":"New function read_data() reads data delimited file, columns read character type. New function prepare_data() validates dataframe field_types specification, prepares aggregation. create_report() accepts new parameter dataset_description user specify dataset description appear report. export_aggregated_data() function accepts new save_file_prefix parameter. New function field_types_template() generates template code creating field_types specification based existing dataframe, outputs console.","code":""},{"path":"/news/index.html","id":"bug-fixes-and-minor-improvements-0-7-0","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"daiquiri 0.7.0 (2022-04-20)","text":"Fixed ALL_FIELDS_COMBINED calculated field rowsumming NAs incorrectly. Fixed plots failing values missing. Fixed log_message() trying write different log file called Rmd folder (relative path used). Made ‘[DUPLICATES]’ ‘[ALL_FIELDS_COMBINED]’ reserved names data fields. Allow column names supplied dataframe contain special characters. Reduced real estate top report. Removed datatype column fixed validation warnings total Source data tab report. Updated example data. Added validation checks user-supplied params. Added CITATION file.","code":""},{"path":"/news/index.html","id":"daiquiri-061-2022-02-23","dir":"Changelog","previous_headings":"","what":"daiquiri 0.6.1 (2022-02-23)","title":"daiquiri 0.6.1 (2022-02-23)","text":"Beta release. Complete list functions exported: aggregate_data() create_report() accepts either dataframe csv filename first parameter. may change future. export_aggregated_data() field_types() ft_categorical() ft_datetime() ft_freetext() ft_ignore() ft_numeric() ft_simple() ft_timepoint() ft_uniqueidentifier() load_data() accepts either dataframe csv filename first parameter. may change future. log_close() log_initialise() report_data()","code":""}]
